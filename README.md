<center><a href="https://dcatkth.github.io/"><img src="research.jpg" width="1400" height="165" align="center"></a></center>

# The Course Title
Reading Course on **Advanced Topics in Distributed Systems**

---
# The Course Content
<p align="justify">
This course is a graduate reading course that will cover the advanced topics in distributed systems, including but not limited to distributed learning, gossip-based learning, graph neural networks, and large scale graph processing. Every participant should find their own relevant research papers, read and analyze their contributions, give a presentation on the material and actively contribute to the group discussions, as well as write a short report on the selected papers. This course is given in the distributed computing group at KTH (<a href="https://dcatkth.github.io/">DC@KTH</a>).
</p>

---
# Intended Learning Outcomes (ILO)
<p align="justify">
After the course the student will be able to discuss, analyze, present, and critically review the very latest research advancements in the areas of distributed systems and learning and make connections to knowledge in related fields. The student will also be able to assess and evaluate new emerging trends as well as to identify the need for further knowledge in the field.
</p>

---
# Course Disposition
<p align="justify">
The course is organized as a reading course. Each student will be required to perform the following tasks:
<ul>
<li><p align="justify"><b>Task 1:</b> identity your relevant research literature under the topic of distirbuted systems, with a focus on distributed learning, gossip-based learning, graph neural networks, large scale graph processing, or similar. Scan the related literature and select three papers that you would like to review. It would be better that the papers tackle the same problem, or are related to the application of the same discipline/approach to different problems. The key point to observe is that the selected papers share some common ground based on which they could be compared against each other.</p></li>
<li><p align="justify"><b>Task 2:</b> write a short justification paragraph to explain your choice of the selected papers. Note that, at this stage, you are not required to read the papers in detail. The paragraph should mostly focus on why you are more interested in the selected focused topic, and how you think your selected papers relate to it (e.g., they address the same research question, they apply different approaches to the same problem).</li>
<li><p align="justify"><b>Task 3:</b> carefully read, analyze, and compare the selected papers to prepare an oral presentation. The presentation should not only present what is in the papers, but mostly contrast and compare their approaches, contributions, and shortcomings, possibly getting/giving insights on related future research. The presentation should be delivered during one of our regular seminar sessions.</li>
<li><p align="justify"><b>Task 4:</b> write a critical review of the papers that covers in particular the summary of contributions, solutions, significance, and technical/experimental quality.</li>
<li><p align="justify"><b>Task 5:</b> choose one of your peers presentations to oppose. You will need to read the papers as well and have a general understanding of their content, contributions, and possible noticed limitations. You have to attend the presentation of your opponent, and to take careful notes on how you perceived its quality, both in terms of content, suitability of the chosen papers and the links between them, and quality of presenting.</li>
<li><p align="justify"><b>Task 6:</b> deliver a written report reviewing your opponent’s work. The review should present objective arguments on what you think are the strengths and weaknesses of the opposed presentation. The report should clearly explain why or why not you think that the selected papers fit within the course’s topic, how the presentation has been fair to explaining the content of the papers, as well as what were the presentations strong points and possible shortcomings.</li>
<li><p align="justify"><b>Task 7:</b> a minimum of 75% attendance in seminars.</li>
</ul>
</p>

# Papers and Schedule
**November 4, 2019 - <a href="mailto:zainabab@kth.se">Zainab Abbas</a>** (opponent: Sana Imtiaz)
* A Deep Learning Framework for Graph Partitioning [[pdf](/papers/A%20Deep%20Learning%20Framework%20for%20Graph%20Partitioning.pdf)]
* Device Placement Optimization with Reinforcement Learning [[pdf](/papers/Device%20Placement%20Optimization%20with%20Reinforcement%20Learning.pdf)]
* Streaming Graph Partitioning for Large Distributed Graphs [[pdf](/papers/Streaming%20Graph%20Partitioning%20for%20Large%20Distributed%20Graphs.pdf)]
* [[justification](/justification/zainab.txt)] [[slides](/slides/zainab_20191104.pdf)]

**November 11, 2019 - <a href="mailto:klasseg@kth.se">Klas Segeljakt</a>** (opponent: Max Meldrum)
* Structured Streaming: A Declarative API for Real-Time Applications in Apache Spark [[pdf](/papers/Structured%20Streaming:%20A%20Declarative%20API%20for%20Real-Time%20Applications%20in%20Apache%20Spark.pdf)]
* State Management in Apache Flink [[pdf](/papers/State%20Management%20in%20Apache%20Flink.pdf)]
* Consistent Regions: Guaranteed Tuple Processing in IBM Streams [[pdf](/papers/Consistent%20Regions:%20Guaranteed%20Tuple%20Processing%20in%20IBM%20Streams.pdf)]
* [[justification](/justification/klas.txt)] [[slides-pdf](/slides/klas_20191111.pdf)] [[slides-pptx](/slides/klas_20191111.pptx)]

**November 18, 2019 - <a href="mailto:sanaim@kth.se">Sana Imtiaz</a>** (opponent: Zainab Abbas)
* Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data [[pdf](/papers/Semi-supervised%20Knowledge%20Transfer%20for%20Deep%20Learning%20from%20Private%20Training%20Data.pdf)]
* SecureML: A System for Scalable Privacy-Preserving Machine Learning [[pdf](/papers/SecureML:%20A%20System%20for%20Scalable%20Privacy-Preserving%20Machine%20Learning.pdf)]
* Chiron: Privacy-preserving Machine Learning as a Service [[pdf](/papers/Chiron:%20Privacy-preserving%20Machine%20Learning%20as%20a%20Service.pdf)]
* [[justification](/justification/sana.txt)] [[slides](/slides/sana_20191118.pdf)]

**November 25, 2019 - <a href="mailto:moritz@logicalclocks.com">Moritz Meister</a>** (opponent: Stefanos Antaris)
* BOHB: Robust and Efficient Hyperparameter Optimization at Scale [[pdf](/papers/BOHB:%20Robust%20and%20Efficient%20Hyperparameter%20Optimization%20at%20Scale.pdf)]
* Massively Parallel Hyperparameter Tuning [[pdf](/papers/Massively%20Parallel%20Hyperparameter%20Tuning.pdf)]
* Population Based Training of Neural Networks [[pdf](/papers/Population%20Based%20Training%20of%20Neural%20Networks.pdf)]
* [[justification](/justification/moritz.txt)] [[slides](/slides/moritz_20191125.pdf)]

**December 16, 2019 - <a href="mailto:lodovico@kth.se">Lodovico Giaretta</a>** (opponent: Negar Safinianaini)
* Gated Graph Sequence Neural Networks [[pdf](/papers/Gated%20Graph%20Sequence%20Neural%20Networks.pdf)]
* Semi-Supervised Classification with Graph Convolutional Networks [[pdf](/papers/Semi-Supervised%20Classification%20with%20Graph%20Convolutional%20Networks.pdf)]
* Inductive Representation Learning on Large Graphs [[pdf](/papers/Inductive%20Representation%20Learning%20on%20Large%20Graphs.pdf)]
* [[justification](/justification/lodovico.txt)] [[slides](/slides/lodovico_20191216.pdf)]

**January 29, 2020 - <a href="mailto:tianzew@kth.se">Tianze Wang</a>** (opponent: Sina Sheikholeslami)
* Exploring Hidden Dimensions in Parallelizing Convolutional Neural Networks [[pdf](/papers/Exploring%20Hidden%20Dimensions%20in%20Parallelizing%20Convolutional%20Neural%20Networks.pdf)]
* Beyond Data and Model Parallelism for Deep Neural Networks [[pdf](/papers/Beyond%20Data%20and%20Model%20Parallelism%20for%20Deep%20Neural%20Networks.pdf)]
* Priority-based Parameter Propagation for Distributed DNN Training [[pdf](/papers/Priority-based%20Parameter%20Propagation%20for%20Distributed%20DNN%20Training.pdf)]
* [[justification](/justification/tianze.txt)] [[slides](/slides/tianze_20200129.pdf)]

**February 5, 2020 - <a href="mailto:mmeldrum@kth.se">Max Meldrum</a>** (opponent: Klas Segeljakt)
* MillWheel: Fault-Tolerant Stream Processing at Internet Scale [[pdf](/papers/MillWheel:%20Fault-Tolerant%20Stream%20Processing%20at%20Internet%20Scale.pdf)]
* Naiad: A Timely Dataflow System [[pdf](/papers/Naiad:%20A%20Timely%20Dataflow%20System.pdf)]
* Ray: A Distributed Framework for Emerging AI Applications [[pdf](/papers/Ray:%20A%20Distributed%20Framework%20for%20Emerging%20AI%20Applications.pdf)]
* [[justification](/justification/max.txt)] [[slides](/slides/max_20200205.pdf)]

**February 12, 2020 - <a href="mailto:sinash@kth.se">Sina Sheikholeslami</a>** (opponent: Moritz Meister and David Daharewa Gureya)
* Supporting Very Large Models using Automatic Dataflow Graph Partitioning [[pdf](/papers/Supporting%20Very%20Large%20Models%20using%20Automatic%20Dataflow%20Graph%20Partitioning.pdf)]
* Placeto: Learning Generalizable Device Placement Algorithms for Distributed Machine Learning [[pdf](/papers/Placeto:%20Learning%20Generalizable%20Device%20Placement%20Algorithms%20for%20Distributed%20Machine%20Learning.pdf)]
* GDP: Generalized Device Placement for Dataflow Graphs [[pdf](/papers/GDP:%20Generalized%20Device%20Placement%20for%20Dataflow%20Graphs.pdf)]
* [[justification](/justification/sina.txt)] [slides]

**February 19, 2020 - <a href="mailto:spozzoli@kth.se">Susanna Pozzoli</a>** (opponent: Lodovico Giaretta)
* Frequent Subgraph Mining Based on Pregel [[pdf](/papers/Frequent%20Subgraph%20Mining%20Based%20on%20Pregel.pdf)]
* Large-Scale Frequent Subgraph Mining in MapReduce [[pdf](/papers/Large-Scale%20Frequent%20Subgraph%20Mining%20in%20MapReduce.pdf)]
* SparkFSM: A Highly Scalable Frequent Subgraph Mining Approach using Apache Spark [[pdf](/papers/SparkFSM:%20A%20Highly%20Scalable%20Frequent%20Subgraph%20Mining%20Approach%20using%20Apache%20Spark.pdf)]
* [[justification](/justification/susanna.txt)] [slides]

**February 26, 2020 - <a href="mailto:antaris@kth.se">Stefanos Antaris</a>** (opponent: Susanna Pozzoli)
* Deep Decentralized Multi-task Multi-Agent Reinforcement Learning under Partial Observability [[pdf](/papers/Deep%20Decentralized%20Multi-task%20Multi-Agent%20Reinforcement%20Learning%20under%20Partial%20Observability.pdf)]
* Multi-Agent Adversarial Inverse Reinforcement Learning [[pdf](/papers/Multi-Agent%20Adversarial%20Inverse%20Reinforcement%20Learning.pdf)]
* Fully Decentralized Multi-Agent Reinforcement Learning with Networked Agents [[pdf](/papers/Fully%20Decentralized%20Multi-Agent%20Reinforcement%20Learning%20with%20Networked%20Agents.pdf)]
* [[justification](/justification/stefanos.txt)] [slides]

<!--
**February 24, 2020 - <a href="mailto:negars@kth.se">Negar Safinianaini</a>** (opponent: Amir Hossein Rahnama)
* zipHMMlib: A Highly Optimised HMM Library Exploiting Repetitions in the Input to Speed Up the Forward Algorithm [[pdf](https://github.com/dcatkth/readinggroup/blob/master/papers/zipHMMlib:%20A%20Highly%20Optimised%20HMM%20Library%20Exploiting%20Repetitions%20in%20the%20Input%20to%20Speed%20Up%20the%20Forward%20Algorithm.pdf)]
* Fast Bayesian Inference of Copy Number Variants using Hidden Markov Models with Wavelet Compression [[pdf](/papers/Fast%20Bayesian%20Inference%20of%20Copy%20Number%20Variants%20using%20Hidden%20Markov%20Models%20with%20Wavelet%20Compression.PDF)]
* Spectral Learning of Mixture of Hidden Markov Models [[pdf](/papers/Spectral%20Learning%20of%20Mixture%20of%20Hidden%20Markov%20Models.pdf)]
* [[justification](/justification/negar.txt)] [slides]
-->

**March 4, 2020 - <a href="mailto:daharewa@kth.se">David Daharewa Gureya</a>** 
* CoPart: Coordinated Partitioning of Last-Level Cache and Memory Bandwidth for Fairness-Aware Workload Consolidation on Commodity Servers [[pdf](/papers/CoPart:%20Coordinated%20Partitioning%20of%20Last-Level%20Cache%20and%20Memory%20Bandwidth%20for%20Fairness-Aware%20Workload%20Consolidation%20on%20Commodity%20Servers.pdf)]
* PARTIES: QoS-Aware Resource Partitioning for Multiple Interactive Services [[pdf](/papers/PARTIES:%20QoS-Aware%20Resource%20Partitioning%20for%20Multiple%20Interactive%20Services.pdf)]
* SWAP: Effective Fine-Grain Management of Shared Last-Level Caches with Minimum Hardware Support [[pdf](/papers/SWAP:%20Effective%20Fine-Grain%20Management%20of%20Shared%20Last-Level%20Caches%20with%20Minimum%20Hardware%20Support.pdf)]
* [[justification](/justification/david.txt)] [slides]

**March 11, 2020 - <a href="mailto:arahnama@kth.se">Amir Hossein Rahnama</a>** (opponent: Tianze Wang)
* This Looks Like That: Deep Learning for Interpretable Image Recognition [[pdf](/papers/This%20Looks%20Like%20That:%20Deep%20Learning%20for%20Interpretable%20Image%20Recognition.pdf)]
* Interpretable Image Recognition with Hierarchical Prototypes [[pdf](/papers/Interpretable%20Image%20Recognition%20with%20Hierarchical%20Prototypes.pdf)]
* Deep Learning for Case-Based Reasoning through Prototypes: A Neural Network that Explains Its Predictions [[pdf](/papers/Deep%20Learning%20for%20Case-Based%20Reasoning%20through%20Prototypes:%20A%20Neural%20Network%20that%20Explains%20Its%20Predictions.pdf)]
* [[justification](/justification/amir.txt)] [slides]

---
# Contact
Contact [Amir H. Payberah](https://payberah.github.io/) if you have any question.

<!--
# Enrollment
Those who are interested in registering for this course and gaining credits, please make sure to have your name on the following list before September 30, 2019: [link](https://docs.google.com/spreadsheets/d/1dLaYwNC9rXUTi-UcTtT_07dEMjo5xVUVvZ25G1v1Pz4/edit?usp=sharing)
-->
